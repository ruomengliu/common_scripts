{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas.core.algorithms as algos\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More care needs to go into a sorting function than one might expect. \n",
    "# The main complication arises due to non-unique bin edges. \n",
    "# I am not aware of a built-in method in Python to do exactly what is in the script, so I wrote this function myself.\n",
    "\n",
    "\n",
    "# The in_df dataframe should contain only one cross-section.\n",
    "# char is the characteristic to sort on.\n",
    "# group is the intended number of bins to sort into.\n",
    "# nyse is a binary variable that tells the function whether to use all or just NYSE firms to determine the bin edges/breakpoints.\n",
    "# The function keeps the original dataframe intact, and adds a column that is by default named a+'_sort,' where a is the name of the variable based on which observations are sorted into bins.\n",
    "\n",
    "\n",
    "# Example:\n",
    "# df is a panel dataframe of firm-month observations.\n",
    "# To sort firms by date into quintiles based on NYSE breakpoints on some characteristic a, do:\n",
    "# df = df.groupby(['date']).apply(xs_CharSort, a, 5, True))\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# When bins are created wrt to the entire cross-section, simply replace True with False.\n",
    "\n",
    "\n",
    "def xs_CharSort(in_df, char, group, nyse):\n",
    "    df = in_df.copy()# I make this (deep) copy here so that I do not change the input dataframe.\n",
    "    if nyse==True:\n",
    "        U_bins = df[(pd.notnull(df[char])) & (df['exchcd']==1.0)].copy() # U_bins is used for getting breakpoints, or bin edges.\n",
    "    else:\n",
    "        U_bins = df[pd.notnull(df[char])].copy()\n",
    "    try:\n",
    "        bins = pd.qcut(U_bins[char], group, retbins=True)[1]\n",
    "        bins[bins==bins.max()] = np.inf\n",
    "        bins[bins==bins.min()] = -np.inf\n",
    "        ranks = pd.cut(df[char], bins=bins, labels=range(1,group+1), include_lowest=True)\n",
    "        df[char+'_sort'] = ranks\n",
    "    except ValueError:\n",
    "        # This is the case where pd.qcut cannot be directly applied due to non-unique bin edges. The rest of this function deals with this case.\n",
    "        # get bins. Code here is from the source pd.qcut script.\n",
    "        quantiles = np.linspace(0, 1, group+1)\n",
    "        quantile_edges = algos.quantile(U_bins[char], quantiles)\n",
    "        quantile_edges_unique = sorted(list(set(quantile_edges)))\n",
    "        if len(quantile_edges_unique)<len(quantile_edges) and len(quantile_edges_unique)>1 and U_bins[char].nunique()>=group: # if bin edges are not unique, but there is enough heterogeneity in the characteristic value in the cross-section, then do the following.\n",
    "            print(char+' on date '+df['date'].unique()[0]+' has non-unique bin edges.')\n",
    "            d = dict(Counter(quantile_edges)) #d is a list of count how many times each bin edge appears.  \n",
    "            dict_clusters = dict((k,v) for k,v in d.items() if v>1).items()\n",
    "            U_rest = U_bins[~U_bins[char].isin([k for k,v in d.items() if v>1])].copy()\n",
    "            counter=0\n",
    "            while len(quantile_edges_unique)<len(quantile_edges) and len(quantile_edges_unique)>1 and group>1: # while there are clusters, and there are some heterogeneity in char value.\n",
    "                if counter>0:\n",
    "                    print(char+' on date '+df['date'].unique()[0]+' has more than one while set of clusters: counter='+str(counter))\n",
    "                # Find out the values at which observations cluster, and assign cluster values to groups\n",
    "                elif counter==0:\n",
    "                    df['del_'+char+'_sort'] = np.nan # This column is necessary here because I will possibly modify it right below\n",
    "                if U_rest[char].nunique()>group-len(dict_clusters): # if after taking out the cluster values, df still has enough unique char values left to sort into group-len(dict_clusters) number of bins, then do the following.\n",
    "                    for cluster_value, times in dict_clusters:\n",
    "                        indices = [i for i,x in enumerate(quantile_edges) if x==cluster_value] # indices is a list of bin edge indices that have cluster values in the bin edge list\n",
    "                        if cluster_value==max(quantile_edges_unique):# If the cluster_value is the largest bin edge, then all values greater than cluster_value should go into the highest bin.\n",
    "                            mask = (df[char]>=cluster_value) & (df['del_'+char+'_sort'].isnull())\n",
    "                            df.loc[mask, 'del_'+char+'_sort'] = (indices[0]+1)*1000+times+group/100 \n",
    "                        elif cluster_value==min(quantile_edges_unique):# If the cluster_value is the lowest bin edge, then all values greater than cluster_value should go into the lowest bin.\n",
    "                            mask = (df[char]<=cluster_value) & (df['del_'+char+'_sort'].isnull())\n",
    "                            df.loc[mask, 'del_'+char+'_sort'] = (indices[0]+1)*1000+times+group/100 \n",
    "                        else:\n",
    "                            df.loc[df[char]==cluster_value, 'del_'+char+'_sort'] = (indices[0]+1)*1000+times+group/100 # This line puts the cluster value into a bin by itself.\n",
    "                    group = group-len(dict_clusters) # redefine group to be the group number needed after accounting for the clusters.\n",
    "                    try:\n",
    "                        bins = pd.qcut(U_rest[char], group, retbins=True)[1]\n",
    "                        bins[bins==bins.max()] = np.inf\n",
    "                        bins[bins==bins.min()] = -np.inf\n",
    "                        ranks = pd.cut(df[char], bins=bins, labels=range(1,group+1), include_lowest=True)\n",
    "                        df['del_'+char+'_sort_rest'] = ranks\n",
    "                        df['del_'+char+'_sort'] = np.where(df['del_'+char+'_sort'].isnull(), df['del_'+char+'_sort_rest'], df['del_'+char+'_sort'])\n",
    "                        # Now column del_char_sort contains the cluster bins and the rest of the bins. The next block relabel the bins in an ascending order.\n",
    "                        df['del_sort_mean'] = df.groupby(['del_'+char+'_sort'])[char].transform('mean')\n",
    "                        relabel = df.groupby(['del_'+char+'_sort'])[char].mean()\n",
    "                        relabel.sort_values(inplace=True)\n",
    "                        relabel.reset_index(drop=True, inplace=True)\n",
    "                        relabel = pd.DataFrame(relabel).rename(columns={char:'del_sort_mean'})\n",
    "                        relabel[char+'_sort'] = (relabel.index+1).astype(float)\n",
    "                        df = pd.merge(df, relabel, how='left', on=['del_sort_mean'])\n",
    "                        df.drop([i for i in df.columns.values if 'del_' in i], axis=1, inplace=True)\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        quantiles = np.linspace(0, 1, group+1)\n",
    "                        quantile_edges = algos.quantile(U_rest[char], quantiles)\n",
    "                        quantile_edges_unique = sorted(list(set(quantile_edges)))\n",
    "                        # print(quantile_edges, quantile_edges_unique, len(quantile_edges_unique)==len(quantile_edges))\n",
    "                        d = dict(Counter(quantile_edges)) #d is a list of counts of how many times each bin edge appears.\n",
    "                        dict_clusters = dict((k,v) for k,v in d.items() if v>1).items()\n",
    "                        U_rest = U_rest[~U_rest[char].isin([k for k,v in d.items() if v>1])].copy()\n",
    "                        U_bins = U_rest.copy()\n",
    "                        counter+=1\n",
    "                        continue\n",
    "                else: # if there is insufficient cross-sectional heterogeneity in char values, then do not sort.\n",
    "                    df[char+'_sort'] = np.nan\n",
    "                    df.drop([i for i in df.columns.values if 'del_' in i], axis=1, inplace=True)\n",
    "                    break\n",
    "        else: # if there is only one value in quantile_edges_unqiue, then do not sort. This is the last of the situations that need to be addressed.\n",
    "            # print(char+' on date '+df['date'].unique()[0]+' has insufficient cross-sectional heterogeneity in char values')\n",
    "            df[char+'_sort'] = np.nan\n",
    "    df[char+'_sort'] = df[char+'_sort'].astype(float)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fama-MacBeth regression\n",
    "# The output is a dataframe of parameter estimates from the first stage cross-sectional regressions.\n",
    "# df should contain only one cross-section.\n",
    "# chars is a list of RHS variables in the cross-sectional regressions.\n",
    "# method is either 'OLS' or 'WLS.' This refers to the type of cross-sectional regression.\n",
    "# If WLS, then the current built-in weight is the variable 'met_1.' Change this to another weight variable if needed.\n",
    "\n",
    "\n",
    "# Example:\n",
    "# res = df.groupby(['date']).apply(FM_Regression,\n",
    "#                                    dep_var=dep_var,\n",
    "#                                    chars=FM_RHS,\n",
    "#                                    method='OLS')\n",
    "# res.reset_index(level=1, drop=True, inplace=True)\n",
    "\n",
    "\n",
    "def FM_Regression(df, dep_var, chars, method):\n",
    "    df2 = df.dropna(subset=[dep_var]+chars,how='any').copy()\n",
    "    min_char_unique = []\n",
    "    for char in chars:\n",
    "        min_char_unique.append(df2[char].nunique())\n",
    "    if len(df2)>=25 and min(min_char_unique)>=2:\n",
    "        for char in chars:\n",
    "            df2[char] = (df2[char] - df2[char].mean()) / df2[char].std()\n",
    "        X = df2[chars]\n",
    "        X = add_constant(X)\n",
    "        Y = df2[dep_var]\n",
    "        if method=='OLS':\n",
    "            result = OLS(Y,X, missing='drop').fit()\n",
    "        elif method=='WLS':\n",
    "            W = df2['met_1']\n",
    "            result = WLS(Y,X, weights=W, missing='drop').fit()\n",
    "        estimates = result.params.values.reshape(1,len(chars)+1)\n",
    "        return pd.DataFrame(estimates,columns=['Intercept']+chars)\n",
    "    else:\n",
    "        estimates = np.array([[np.nan]*(1+len(chars))])\n",
    "        return pd.DataFrame(estimates,columns=['Intercept']+chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate standard error from using the xs coefficient estimates from the FM_regression function.\n",
    "# Adopted from https://github.com/thouis/pandas/blob/master/pandas/stats/fama_macbeth.py\n",
    "# var is a single variable wrt which you want to compute standard error. \n",
    "# I put it in a list as input just so list() is not required inside the function.\n",
    "\n",
    "# Example:\n",
    "# var_stderr = FM_stderr(in_df=res, var=[i], nw_lags=12)[0]\n",
    "\n",
    "def FM_stderr(in_df, var, nw_lags):\n",
    "    df = in_df.dropna(subset=var, how='any').copy()\n",
    "    T = len(df)\n",
    "    coeffs = df[var].copy()\n",
    "    B = coeffs - coeffs.mean(0)\n",
    "    C = np.dot(B.T, B) / T\n",
    "    if nw_lags is not None and nw_lags>=1:\n",
    "        for i in range(1, nw_lags+1):\n",
    "            cov = np.dot(B[i:].T, B[:(T - i)]) / T\n",
    "            weight = i / (nw_lags + 1)\n",
    "            C += 2 * (1 - weight) * cov\n",
    "    stderr = np.sqrt(np.diag(C)) / np.sqrt(T)\n",
    "    return stderr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
